# User Uploads From DropBox

## For the cellxgene Data Portal

**Authors:** [Trent Smith](mailto:trent.smith@chanzuckerberg.com)

**Approvers:** [Arathi Mani](mailto:arathi.mani@chanzuckerberg.com), [Timmy Huang](mailto:thuang@chanzuckerberg.com),
[Marcus Kinsella](mailto:mkinsella@chanzuckerberg.com), [Eduardo Lopez](mailto:elopez@chanzuckerberg.com), [Brian Raymor](mailto:braymor@chanzuckerberg.com)
[Ryan King](mailto:ryan.king@chanzuckerberg.com), [Brian McCandless](bmccandless@chanzuckerberg.com)

## tl;dr

A user can upload files, up to 30GB in size, to Data Portal from a DropBox link.

## Glossary

- **Shared Link** - A link from DropBox that gives the bearer of that link access to private data.
- **DP(Data Portal)** - The cellxgene Data Portal.
- **Small File** - a file < 100 MB in size.
- **Large File** - a file > 100 MB in size.
- **Collection** - a collection of datasets uploaded and published by a users.
- **Submission** - a collection that is going through the publishing process.

## Problem Statement | Background

One of the primary goals of the DP is to allow users to upload their own files. These files consist of datasets.
These datasets can vary in size from a few MB to several GB. Users can provide a shared link from their DropBox that
is used to upload directly to S3.

This specification does not cover the validation of the data or the process for a user to complete a submission.

## Product Requirements

1. A user can upload a file to DP using a shared link from their Dropbox.
1. A user can initiate an upload from the DP Browser App.
1. A user can abort an upload from the DP Browser App.
1. A user can check the status and progress of their upload from the DP Browser App.
1. A user can restart a failed upload from the DP Browser App.
1. The user must be authenticated before uploading.
1. A user who has not yet agreed to the Portal policies must not be able to upload their file.
1. Supports the upload of AnnData, Loom, or Seurat v3 RDS. See [Reference](https://github.com/chanzuckerberg/corpora-data-portal/blob/main/backend/schema/corpora_schema.md#implementations).

### Nonfunctional requirements

1. Cloud to Cloud uploads will retry if they fail. Retry at least 5 times with an exponential back off.
1. A user can upload a single file up to 30GB in size.
1. Shared links referring to more than one file will be rejected.
1. The status of an upload must be tracked and queryable.

## Detailed Design | Architecture | Implementation

### Cloud to Cloud Upload Flow

Cloud to cloud upload requires the user to provide a shareable link for a file on their DropBox from which DP can upload
the file to the S3 bucket. Now to walk through the cloud to cloud flow, see figure below.

![Cloud to Cloud](https://app.lucidchart.com/publicSegments/view/e1e72143-c108-4c7f-bf6c-053d98256c1f/image.png)
**Figure:** Architecture of a user uploading from their cloud to our cloud.

#### 1. Get Share Link

The user must generate a sharable link from their DropBox. Sharable links should only point to a single file, not a
directory of files.

#### 2. Share Link

Using the browser App, make a _POST {submission_id}/upload/link_request to upload the file
by providing the sharable link in the request body. The response will contain the_dataset_id_, that can be used to
check the status of the upload

#### 3. Sanitize

To prevent [server side request forgery](https://owasp.org/www-community/attacks/Server_Side_Request_Forgery) the
incoming shared link will be sanitized before queuing for download. The links generated by DropBox are often not direct
links to the files and need to be parsed into the correct shape for downloading. The correct URL will be generated
as part of the sanitization process. The backend that runs this should also be given minimal privileges to further protect from
server-side request forgery. How to download from a shared link on [Dropbox](https://help.dropbox.com/files-folders/share/force-download).

The DropBox download link can be generated by modifying the query parameter of the shared link. A shared link from
DropBox looks like _<https://www.dropbox.com/s/{folder_id}/{file_id}url?dl=0>_. By changing the query parameter `dl=0` to
`dl=1`, the shared file can be downloaded.

The URL will be sanitized by verifying it matches the expected format using regex. The URL must also refer to the correct DropBox domain. The URL will be rejected if it does not match.

The backend will also create an entry in the upload table using the _dataset_id_as the key, and set the status to_Waiting_.

#### 4. Queue

The queue stores the links that need to be processed. It buffers the downstream servers from download requests. This
allows us to spin up servers as needed. It also gives us the ability to retry a download if one of the servers to fails
upload the file.

##### Why Might the Upload Fail

- The downloading server crashed.
- DropBox experienced an outage
- The user has reached their [daily bandwidth allowance](https://www.dropbox.com/plans?trigger=nr) from their DropBox. [more info](https://help.dropbox.com/files-folders/share/banned-links).

#### 5. Start an Instance to Download File

An EC2 instance, docker container, or AWS lambda can be used to complete this task. Once a computing resource is spawned,
it will change the upload status to _Uploading_.

#### 6. Stream File

The shared link is used to stream the file from one cloud to another. The most time will be spent streaming the file
from the DropBox to S3. DropBox offers a way to validate the integrity of a download
by providing a hash of the data, for example [Dropbox Content Hash](https://www.dropbox.com/developers/reference/content-hash).
As the data is streaming from the CSP, the hash of the file will be calculated as it arrives, and again as it's
uploaded to S3. The [Content-MD5 header](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html#API_PutObject_RequestSyntax)
will be used when uploading to S3 to verify the integrity of each chunk. The AWS SDK will be useful in simplifying the
upload to S3. For calculating the checksum while streaming, [checksumming_io](https://github.com/HumanCellAtlas/checksumming_io)
can be used.

Once the upload is complete, the download job will be removed from the queue. If the final hash does not match, then
the upload process will be retried.

While streaming the file, the computing resource will periodically check if the upload has been canceled using the upload
table. If the upload status is _Cancel Pending_, the computing resource will stop the stream, delete any data that has been uploaded,
mark the job as complete in the queue, and set the status of the upload to cancelled.

Where possible, the progress of the upload will be calculated and updated in the upload table.

#### 7. Retry

If the upload fails for any reason, the computing resource will mark the status of the upload as _Waiting_, and return the upload
job to the queue. The number of times an upload job is retried will be adjustable. Initially the number of retries will
be 5 times, with exponential back off.

#### 8. Failed Uploads

If an upload fails enough times, it will be removed from the download queue. The upload status will change to failed.

#### 9. Validation Hand Off

Once the upload completes successfully, an [S3 event](https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html)
is emitted by the upload bucket to inform the validation service to begin processing.

### Components

This section describes the purpose of components that make up the upload design.

#### Data Portal Browser App

The DP Browser App is the frontend of the system. It is the primary method users have to interact with the Data Portal.
It will be used to authenticate the user and interact with the DP Backend to upload files.

#### Data Portal Backend

The DP Backend is responsible for handling API requests. It will enqueue shared links for downloading, and update the
upload status. This additional functionality can be built into the existing DP Backend Lambda.

The backend will also be responsible for performing a simple validation of the file being uploaded. The validation is
performed during the initial request to upload. The validation consists of verifying the extension of the file is a [supported type](https://github.com/chanzuckerberg/corpora-data-portal/blob/main/backend/schema/corpora_schema.md#implementations).

#### Upload Queue

The upload queue is an [AWS SQS](https://aws.amazon.com/sqs/) that will track pending upload jobs.
The upload jobs are used by the upload service to know where to download the file from before uploading them to S3.
For the first implementation, the job sleep time will be 15min. Eventually, the job sleep time will depend on the size
of the file being uploaded.

Cloudwatch events can be used to monitor the SQS length. Additional computing resource can be spawned in response to an
increase in jobs in the queue.

##### Upload Job Entry

These are the fields that will be in the upload job placed in the queue:

- **link** - the link to file to download. Use to retrieve the file from DropBox.
- **submission_id** - identifies the submission. Used to determine the storage location.
- **dataset_id** - identifies the dataset. Used to determine the storage location.
- **file_name** - the name of the file being downloaded.

#### Upload Service

A computing resource within the AWS cloud. It will receive upload jobs from the upload queue. Those jobs contain
the information needed by the computing resource to download a file from DropBox and upload it to S3. The computing resource
will also verify the integrity of the download and upload. The computing resource will also change the state of the
upload in the Upload table as needed.

The computing resource will poll the upload table for a _Cancel Pending_ status. If the _Cancel Pending_ status is set, the
computing resource will delete any data uploaded to S3, remove the upload job from the queue, and change the status of
the upload to _Canceled_.

If an unrecoverable error occurs while processing the upload, the computing resource will return the upload job to the
queue and update the upload table.

If an error is encountered that cannot be fixed by retrying, the upload status will change to failed. The error can be
returned using the _GET submission/{submission_id}/upload_endpoint with the_dataset_id_ associated with the upload.

##### Upload Table

The upload table is a DynamoDB table that tracks the current status of uploads.

DynamoDB was favored over adding additional fields to the existing Database. The upload table will be updated and queried
frequently by the browser app and the upload service. The browser app will query periodically for upload progress.
The upload service will query the upload table for a _Cancel Pending_ status, and to update the progress and status of
the upload. With the current database cached behind CloudFront, the upload service would constantly need to clear the
CloudFront cache if the existing database was used.

The table will have the following columns:

- dataset_id - primary key. This field is used to identify the upload job.
- status - the current state of the upload. This field is updated to inform the user of the current state of the upload.
- progress - the current progress of the upload. While the file is actively being uploaded the progress of the upload
  will be updated. If the size of the file to be uploaded is known, the progress will indicate how much has been
  uploaded over the total file size. If the total file size is not known, the progress will not be updated. This field is
  returned using `GET submission/{submission_id}/upload`.
- owner - the id of the user who owns this collection. This field is used to enforce if the user authorized to view the
  remaining fields.
- message - error message if the upload fails. This field is updated when an error occurs that should be visible to the
  owner.

#### Upload Bucket

The upload bucket is a separate bucket. It is used to stage files
before moving to the main bucket. Uploading files to a staging buckets makes it easier to clean up canceled, partial,
or otherwise abandoned uploads, without accidentally deleting files actively being served by the Data Portal. Central
Infra also provides a [bucket template](https://github.com/chanzuckerberg/cztack/tree/main/aws-s3-private-bucket) to help
manage stale multipart uploads.

S3 event can be used to trigger additional processing such as validation, once the upload has completed. The additional
processing is outside the scope of this RFC.

Files are uploaded to {collection_id}/{dataset_id}/{file_name} in the upload bucket.

### Data Portal APIs

The DP Browser App will use these endpoints to interact with the Backend using these APIs. This section describes the new
endpoints and how they are used. These are the new API endpoints needed to complete an upload.

These APIs replace the [PUT /v1/submission/{project_uuid}/file](https://docs.google.com/document/d/1d8tv2Ub5b3E7Il85adOAUcG8P05N6UBZJ3XbhJSRrFs/edit#heading=h.3hln6w2kzoyt)
section.

#### POST submission/{submission_id}/upload/link

An authenticated user can upload a file from a shared link to a dataset in their submission.

If the upload is in an error state, this endpoint can be used with the _dataset_id_ to restart the submission. If a new
link is provided, the new link will be used. If the submission is not in an error state, this endpoint will
return an error.

Starting an upload causes the database to be updated with a new dataset. The dataset will set the initial state of the upload
to _Waiting_ once it has been accepted and is in the upload queue.

**Request:**

| Parameter             | Description                                                                    |
| --------------------- | ------------------------------------------------------------------------------ |
| Link                  | a shared link to the file                                                      |
| submission_id         | identifies the submission                                                      |
| _Optional_ dataset_id | identifies the dataset being uploaded. Used to check the status of the upload. |

**Response:**

| Key        | Description                                                               |
| ---------- | ------------------------------------------------------------------------- |
| dataset_id | identifies the dataset the file will be associated with after validation. |
| status     | Provides the current status of the upload                                 |

**Error Responses:**

| Code | Description                                                                                 |
| ---- | ------------------------------------------------------------------------------------------- |
| 401  | if _dataset_id_or_submission_id_ does not exist, or if the user does not own the submission |
| 400  | if the file type is invalid.                                                                |
| 409  | if there is an existing submissions in progress with the same _dataset_id_.                 |
| 413  | if the links refer to a file that exceeds the max size allowed.                             |

#### GET submission/{submission_id}/upload

Checks the status of an existing upload job.

**Request:**

| Parameter     | Description                          |
| ------------- | ------------------------------------ |
| submission_id | identifies the submission            |
| dataset_id    | identifies the dataset to be queried |

**Response:**

| Key      | Description                                   |
| -------- | --------------------------------------------- |
| status   | Provides the current status of the upload.    |
| message  | If an error occurred, the message shows here. |
| progress | The current progress of the upload.           |

**Error Responses:**

| Code | Description                                                                                                               |
| ---- | ------------------------------------------------------------------------------------------------------------------------- |
| 401  | if _dataset_id_or_submission_id_ does not exist, or if the user does not own the submission or upload in-progress upload. |
| 400  | if the parameters supplied are invalid.                                                                                   |

#### DELETE submission/{submission_id}/upload

Cancels an existing upload job. Any data that has started to upload is removed, and the upload status is changed to
_Cancel Pending_ until the job has been cleared up.

**Request:**

| Parameter     | Description                           |
| ------------- | ------------------------------------- |
| submission_id | identifies the submission             |
| dataset_id    | identifies the dataset being canceled |

**Response:**

| Key    | Description                                |
| ------ | ------------------------------------------ |
| status | Provides the current status of the upload. |

**Error Responses:**

| Code | Description                                                                                                                     |
| ---- | ------------------------------------------------------------------------------------------------------------------------------- |
| 401  | if _dataset_id_or_submission_id_ does not exist, if the user does not own the submission, or the upload is in state _Uploaded_. |
| 400  | if the parameters supplied are invalid.                                                                                         |

#### Upload Status and State

The API documentation returns a _status_ in response. This status informs the user of the current state of the upload.
An upload follows this state diagram:

![Upload States](https://app.lucidchart.com/publicSegments/view/5fe0858f-eb93-4b64-9842-6f25cad371f3/image.png)
Figure 2: Upload State Diagram

There are several different states that an upload can be in. The table below defines them.

| #   | states         | Description                                                                                 |
| --- | -------------- | ------------------------------------------------------------------------------------------- |
| S0  | Start          | The upload is never actually in this state.                                                 |
| S1  | Waiting        | The upload is enqueued, and waiting for computing resources.                                |
| S2  | Uploading      | The file is actively being uploaded.                                                        |
| S3  | Uploaded       | The upload was completed successfully.                                                      |
| S4  | Failed         | The upload has failed. A new link or file must be uploaded. Any upload progress is deleted. |
| S5  | Cancel Pending | The upload is in the process of being canceled                                              |
| S6  | Canceled       | The upload has been canceled. Any upload progress is deleted.                               |

Once the upload is complete validation can begin.

##### S0 - >S1

An upload moves from S0 to S1 when an upload request has been made, and the system is waiting for resources to begin uploading.

##### S1 - >S2

An upload moves from S1 to S2 when the file is actively being uploaded to S3.

##### S2 -> S5

Once a user requests an upload to be canceled the state changes to _Cancel Pending_. The cancellation may not happen
immediately, therefore the status of the upload is changed to _Cancel Pending_ until the upload has been canceled by the
computing resource processing the job.

##### S5 -> S6

Once the upload has been canceled by the computing resource the state will be _Canceled_.

##### S2 -> S3

The upload has been completed. The state of the upload is _Uploaded_.

##### S2 -> S1

If the upload fails, the job returns to the queue to be retried at a later time. The state changes to _Waiting_.

#### S2 -> S4

The upload job has failed to download after several retries. The state of the upload is _Failed_.

### Test plan

1. Verify that a large file can be uploaded using a shared link from Dropbox.
1. Verify that a small file can be uploaded using a shared link from Dropbox.
1. Verify the upload queue is not cleared until the file is store in S3.
1. Verify a failed upload is marked as _Failed_.
1. Verify an upload is retried if it fails.
1. Verify only the owner can query the status of an upload.
1. Verify only the owner can upload toa dataset to their collection.
1. Verify partial uploads are erased from S3 if they fail or are canceled.
1. Verify a user can start another upload if the original upload failed, or was canceled.
1. Verify a shared folder from DropBox cannot be uploaded.
1. Verify uploaded files are removed from the upload bucket, once they moved to the primary bucket.
1. Verify an uploaded file can be overwritten.
1. Verify only [accepted file types](https://github.com/chanzuckerberg/corpora-data-portal/blob/main/backend/schema/corpora_schema.md#implementations)
   are allowed.
1. Verify multiple independent uploads can occur simultaneously.

### Monitoring and error reporting

- Monitor the length of the upload queue. If it gets too long, then increase the number of computing instances used.
- Log failure on the computing resource.

## Alternatives Considered

### Download using SMB

Dropbox and Google drive support syncing files to a server using their desktop application. This used SMB protocol
to sync your local storage with what is in the cloud. This will allow a user to share a file with us by adding us
to their shared folder. This would cause the file to sync to our system. This approach would solve the download problem,
and at the same time introduce more uploading and cleanup problems. The download to our server would be asynchronous,
which would mean the server needs to be up 100% of the time. Instruction must be provided to users on how to share
their files with our account. The file names would be controlled by the user until they are synced into our server.
From that point, a mapping of files to their file name uploaded to S3 would be needed. Given a cluster of servers, I'm not
certain if the SMB protocol would duplicate the files across all of the servers or if the servers can be instructed which
files to clone.

## References

- [S3 IAM Permissions](https://docs.aws.amazon.com/AmazonS3/latest/dev/list_amazons3.html)
- [AWS S3 Multipart Upload Permissions](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuAndPermissions.html)
- [Original Ticket](https://app.zenhub.com/workspaces/single-cell-5e2a191dad828d52cc78b028/issues/chanzuckerberg/single-cell/25)
- [HCA Upload Service](https://docs.google.com/document/d/1PiO-0ThE7GxQpw7XqkK8P9d8E8xP8K9ssMCR0w4WTd4/edit) - a lot of good ideas and things to consider.
- [Corpora High Level Architecture](https://docs.google.com/document/d/1d8tv2Ub5b3E7Il85adOAUcG8P05N6UBZJ3XbhJSRrFs/edit#heading=h.3hln6w2kzoyt)
- [AWS-S3-upload-integrity](https://stackoverflow.com/questions/42208998/aws-s3-upload-integrity)
- [Diagrams](https://app.lucidchart.com/invitations/accept/81a342d2-c092-4855-a6bc-6320e4f9bea1)
