# User Uploads From Drop Box

## For the cellxgene Data Portal

**Authors:** [Trent Smith](mailto:trent.smith@chanzuckerberg.com)

**Approvers:** [Arathi Mani](mailto:arathi.mani@chanzuckerberg.com), [Timmy Huang](mailto:thuang@chanzuckerberg.com),
[Marcus Kinsella](mailto:mkinsella@chanzuckerberg.com), [Eduardo Lopez](mailto:elopez@chanzuckerberg.com), [Brian Raymore](mailto:braymor@chanzuckerberg.com)
[Sara Gerber](mailto:sara.gerber@chanzuckerberg.com)

## tl;dr

A user can upload files of any size to Data Portal from a DropBox link.

## Glossary

- Shared Link - A link from DropBox that gives the bearer of that link access to private data.
- DP(Data Portal) - The cellxgene Data Portal.
- Small File - a file < 100 MB in size.
- Large File - a file > 100 MB in size.

## Problem Statement | Background

One of the primary goals of the DP is to allow users to upload their own files. These files consist of data sets.
These data sets can vary in size from a few MB to several GB. Users can provide a shared link from their DropBox that
is used to upload directly to S3.

This specification does not cover the validation of the data or the process for a user to complete a submission.

## Product Requirements

1. A user can upload a file to DP using a shared link from their Dropbox.
1. A user can initiate an upload from the DP Browser App.
1. A user can abort an upload from the DP Browser App.
1. A user can check the status of their upload from the DP Browser App.
1. A user can restart a failed upload from the DP Browser App.
1. The user must be authenticated before uploading.
1. A user who has not yet agreed to the Portal policies must not be able to upload their file.

### Nonfunctional requirements

1. Cloud to Cloud uploads will retry if they fail. Retry at least 5 times with an exponential back off.
1. A user can upload a single file up to 30GB in size.
1. Shared links referring to more than one file will be rejected.
1. The status of an upload must be tracked and queryable.

## Detailed Design | Architecture | Implementation

![Upload Design](https://app.lucidchart.com/publicSegments/view/a74f747f-d071-41ea-949f-52f38a98747a/image.png)
Figure 1: The Upload Design

### Cloud to Cloud Upload Flow

Cloud to cloud upload requires the user to provide a shareable link for a file on their DropBox from which DP can upload
the file to the S3 bucket. Now to walk through the cloud to cloud flow, see figure 3.

![Cloud to Cloud](https://app.lucidchart.com/publicSegments/view/e1e72143-c108-4c7f-bf6c-053d98256c1f/image.png)
**Figure 3:** Architecture of a user uploading from their cloud to our cloud.

#### 1. Get Share Link

The user must generate a sharable link from their DropBox. Sharable links should only point to a single file, not a
folder or submission of files.

#### 2. Share Link

Using the browser App, make a \_POST {submission_id}/upload/link_request to upload the file
by providing the sharable link in the request body. The response will contain the dataset_id, that can be used to
check the status of the upload

#### 3. Sanitize

To prevent [server side request forgery](https://owasp.org/www-community/attacks/Server_Side_Request_Forgery) the
incoming shared link will be sanitized before queuing for download. The links generated by DropBox are often not direct
links to the files and need to be parsed into the correct shape for downloading. The correct URL will be generated
as part of the sanitization process. The backend that runs this should also be given minimal privileges to further protect from
server-side request forgery. How to download from a shared link on [Dropbox](https://help.dropbox.com/files-folders/share/force-download).

The backend will also create an entry in the upload table using the dataset_id as the key, and set the status to waiting.

#### 4. Queue

The queue stores the links that need to be processed. It buffers the downstream servers from download requests. This
allows us to spin up servers as needed. It also gives us the ability to retry a download if one of the servers to fails
upload the file.

##### Why might the download fail

- The downloading server ran out of storage.
- The downloading server crashed.
- DropBox experience and outage
- The user has reached their daily bandwidth allowance from their DropBox. [more info](https://help.dropbox.com/files-folders/share/banned-links)

#### 5. Start an Instance to Download File

An EC2 instance, docker container, or AWS lambda can be used to complete this task. Once a compute resource is spawned,
it will change the upload status to uploading.

#### 6. Stream File

The shared link is used to stream the file from one cloud to another. The most time will be spent streaming the file
from the DropBox to S3. DropBox offers a way to validate the integrity of a download
by providing a hash of the data, for example [Dropbox Content Hash](https://www.dropbox.com/developers/reference/content-hash).
As the data is streaming from the CSP, the hash of the file will be calculated as it arrives, and again as it's
uploaded to S3. The [Content-MD5 header](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html#API_PutObject_RequestSyntax)
will be used when uploading to S3 to verify the integrity of each chunk. The AWS SDK will be useful in simplifying the
upload to S3.

Once the upload is complete, the download job will be removed from the queue. If the final hash does not match, then
the upload process will be restarted. The retry logic will be an exponential backoff, with a max retry of 5 times.

While streaming the file, the compute resource will periodically check if the upload has been canceled using the upload
table. If the upload status is cancel pending, the compute resource will stop the stream, delete any data that has been uploaded,
mark the job as complete in the queue, and set the status of the upload to cancelled.

#### 7. Retry

If the upload fails for any reason, the compute resource will mark the status of the upload as waiting, and return the upload
job to the queue.

#### 8. Failed Uploads

If an upload fails enough times, it will be removed from the download queue. The upload status will change to failed.

### Components

This section describes the purpose of components that make up the upload design.

#### Data Portal Browser App

The DP Browser App is the frontend of the system. It is the primary method users have to interact with the Data Portal.
It will be used to authenticate the user and interact with the DP Backend to upload files.

#### Data Portal Backend

The DP Backend is responsible for handling API requests. It will enqueue shared links for downloading, and update the
upload status. This additional functionality can be built into the existing DP Backend Lambda.

The backend will also be responsible for performing validation of the file being uploaded. The validation is
performed during the initial request to upload.

#### Download Queue

The Download Queue is an [AWS SQS](https://aws.amazon.com/sqs/) that will track pending download jobs.
The download jobs are used by the upload service to know where to download the file from before uploading them to S3.

##### Download Job Entry

These are the fields that will be in the download job placed in the queue:

- Link - a to the link to download. Use to retrieve the file from CSP.
- submission_id - identifies the submission. Used to determine the storage location.
- dataset_id - identifies the dataset. Used to determine the storage location
- file_name - the name of the file being downloaded

#### Upload Service

A compute resource within the AWS cloud. It will receive download jobs from the Download Queue. Those jobs contain
the information need by the compute resource to download a file from DropBox and upload it to S3. The computer resource
will also verify the integrity of the download and upload. The compute resource will also change the state of the
upload in the Upload table as needed.

If an error occurs while processing the upload, the compute resource will return the download job to the queue and
update the upload table.

##### Upload Table

The upload table is a DynamoDB table that tracks the current status of uploads. The table will have the following columns:

- dataset_id - primary key
- status - the current state of the upload.
- progress - the current progress of the upload.
- owner - the id of the user who owns this collection.

#### Upload Bucket

The upload bucket is a separate bucket or separate bucket location within our main bucket. It is used to stage files
before moving to the main bucket. Uploading files to a staging buckets makes it easier to clean up canceled, partial,
or otherwise abandoned uploads, without accidentally deleting files actively being served by the Data Portal.

S3 event can be used to trigger additional processing such as validation, once the upload has completed. The additional
processing is outside the scope of this RFC.

### Data Portal APIs

The DP Browser App will use these endpoints to interact with the Backend using these APIs. This section describes the new
endpoints and how they are used. These are the new API endpoints needed to complete an upload.

These APIs replace the [PUT /v1/submission/{project_uuid}/file](https://docs.google.com/document/d/1d8tv2Ub5b3E7Il85adOAUcG8P05N6UBZJ3XbhJSRrFs/edit#heading=h.3hln6w2kzoyt)
section.

#### PUT submission/{submission_id}/upload/link

An authenticated user can upload a file from a shared link to a data set in their submission.

If the upload is in an error state, this endpoint can be used with the dataset_id to restart the submission. If a new
link is provided, the new link will be used. If the submissions are not in an error state or complete, this endpoint
will return the current status of the submission.

Starting an upload causes the database to be updated with a new data set. The dataset will set the initial state of the upload
to waiting once it has been accepted and is in the download queue.

**Request:**

| Parameter             | Description                                                                                                                                           |
| --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| Link                  | a shared link to the file                                                                                                                             |
| submission_id         | identifies the submission                                                                                                                             |
| _Optional_ dataset_id | identifies the dataset being uploaded. Used to check the status of the upload.                                                                        |
| _Optional_ overwrite  | A boolean, indicating if the user would like to overwrite the file that was uploaded. The upload status must be complete for the dataset_id provided. |

**Response:**

| Key        | Description                                                                |
| ---------- | -------------------------------------------------------------------------- |
| dataset_id | identifies the data set the file will be associated with after validation. |
| status     | Provides the current status of the upload                                  |

**Error Responses:**

| Code | Description                                                                                                             |
| ---- | ----------------------------------------------------------------------------------------------------------------------- |
| 401  | if dataset_id or submission_id does not exist, or if the user does not own the submission or upload in-progress upload. |
| 400  | if the file type is invalid                                                                                             |

#### Delete submission/{submission_id}/upload

Cancels an existing upload job. Any data that has started to upload is removed, and the upload status is changed to
cancel pending until the job has been cleared up.

**Request:**

| Parameter     | Description                           |
| ------------- | ------------------------------------- |
| submission_id | identifies the submission             |
| dataset_id    | identifies the dataset being canceled |

**Response:**

| Key    | Description                                |
| ------ | ------------------------------------------ |
| status | Provides the current status of the upload. |

**Error Responses:**

| Code | Description                                                                                                             |
| ---- | ----------------------------------------------------------------------------------------------------------------------- |
| 401  | if dataset_id or submission_id does not exist, or if the user does not own the submission or upload in-progress upload. |
| 400  | if the parameters supplied are invalid.                                                                                 |

#### Upload Status and State

The API documentation returns a _status_ in response. This status informs the user of the current state of the upload.
An upload follows this state diagram:

![Upload States](https://app.lucidchart.com/publicSegments/view/5fe0858f-eb93-4b64-9842-6f25cad371f3/image.png)
Figure 2: Upload State Diagram

There are several different states that an upload can be in. The table below defines them.

| #   | states         | Description                                                                                 |
| --- | -------------- | ------------------------------------------------------------------------------------------- |
| S0  | Start          | The upload is never actually in this state.                                                 |
| S1  | Waiting        | The upload is enqueued, and waiting for computing resources.                                |
| S2  | uploading      | The file is actively being uploaded.                                                        |
| S3  | Completed      | The upload was completed successfully.                                                      |
| S4  | Failed         | The upload has failed. A new link or file must be uploaded. Any upload progress is deleted. |
| S5  | Cancel Pending | The upload is in the process of being canceled                                              |
| S6  | Canceled       | The upload has been canceled. Any upload progress is deleted.                               |

These states will be stores in the upload table until the upload is complete.

##### S0 - >S1

An upload moves from S0 to S1 when an upload request has been made, and the system is waiting for resources to begin uploading.

##### S1 - >S2

An upload moves from S1 to S2 when the file is actively being uploaded to S3.

##### S2 -> S5

Once a user requests an upload to be canceled the state changes to Cancel Pending. The cancellation may not happen
immediately, therefore the status of the upload is changed to cancel pending until the upload has been canceled by the
compute resource processing the job.

##### S5 -> S6

Once the upload has been canceled by the compute resource the state will be canceled.

##### S2 -> S3

The upload has been completed. The state of the upload is complete.

##### S2 -> S1

If the upload fails, the job returns to the queue to be retried at a later time. The state changes to waiting.

#### S2 -> S4

The upload job has failed to download after several retries. The state of the upload is failed.

### Test plan

1. Verify that a large file can be uploaded using a shared link from Dropbox.
1. Verify that a small file can be uploaded using a shared link from Dropbox.
1. Verify the download queue is not cleared until the file is store in S3.
1. Verify a failed download is marked as failed.
1. Verify a download is retried if it fails.
1. Verify only the owner can query the status of an upload.
1. Verify only the owner can upload toa dataset to their collection.
1. Verify partial uploads are erased from S3 if they fail or are canceled.
1. Verify a user can start another upload if the original upload failed, or was canceled.
1. Verify a complete upload cannot be canceled.
1. Verify a shared folder from DropBox cannot be uploaded.
1. Verify uploaded files are removed from the upload bucket, once they moved to the primary bucket.
1. Verify an uploaded file can be overwritten.

### Monitoring and error reporting

- Monitor the length of the Download queue. If it gets too long, then increase the number of compute instances used.
- Log downloading failure on the computing resource.

## [Optional] Alternatives

### Download using SMB

Dropbox and Google drive support syncing files to a server using their desktop application. This used SMB protocol
to sync your local storage with what is in the cloud. This will allow a user to share a file with us by adding us
to their shared folder. This would cause the file to sync to our system. This approach would solve the download problem,
and at the same time introduce more uploading and cleanup problems. The download to our server would be asynchronous,
which would mean the server needs to be up 100% of the time. Instruction must be provided to users on how to share
their files with our account. The file names would be controlled by the user until they are synced into our server.
From that point, a mapping of files to their file name uploaded to S3 would be needed. Given a cluster of servers, I'm not
certain if the SMB protocol would duplicate the files across all of the servers or if the servers can be instructed which
files to clone.

## References

Any relevant documents or prior art should be clearly listed here. Any citation of figures or papers should also be clearly listed here. Of course, they can also be linked throughout the document when directly referenced.

- [S3 IAM Permissions](https://docs.aws.amazon.com/AmazonS3/latest/dev/list_amazons3.html)
- [AWS S3 Multipart Upload Permissions](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuAndPermissions.html)
- [Original Ticket](https://app.zenhub.com/workspaces/single-cell-5e2a191dad828d52cc78b028/issues/chanzuckerberg/single-cell/25)
- [HCA Upload Service](https://docs.google.com/document/d/1PiO-0ThE7GxQpw7XqkK8P9d8E8xP8K9ssMCR0w4WTd4/edit) - a lot of good ideas and things to consider.
- [Corpora High Level Architecture](https://docs.google.com/document/d/1d8tv2Ub5b3E7Il85adOAUcG8P05N6UBZJ3XbhJSRrFs/edit#heading=h.3hln6w2kzoyt)
- [AWS-S3-upload-integrity](https://stackoverflow.com/questions/42208998/aws-s3-upload-integrity)
- [Diagrams](https://app.lucidchart.com/invitations/accept/81a342d2-c092-4855-a6bc-6320e4f9bea1)
